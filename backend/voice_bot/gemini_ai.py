# # import google.generativeai as genai
# # import json
# # import re
# # from dotenv import load_dotenv
# # import os

# # load_dotenv()
# # API_KEY = os.getenv("AIzaSyDUClq0SgIcsHqjtQzpHpr7FcPkyVQMWOM")
# # genai.configure(api_key=API_KEY)
# # model = genai.GenerativeModel(model_name="models/gemini-1.5-flash")

# # PROMPT_TEMPLATE = """
# # ‡§§‡•Å‡§Æ ‡§è‡§ï ‡§∏‡•ç‡§Æ‡§æ‡§∞‡•ç‡§ü ‡§®‡§æ‡§ó‡§∞‡§ø‡§ï ‡§∂‡§ø‡§ï‡§æ‡§Ø‡§§ ‡§™‡•ç‡§∞‡§£‡§æ‡§≤‡•Ä ‡§π‡•ã...  # shortened for brevity
# # Conversation:
# # {conversation_log}
# # """


# # def ask_gemini_followup_or_result(conversation_log):
# #     prompt = PROMPT_TEMPLATE.format(conversation_log=conversation_log)
# #     response = model.generate_content(prompt)
# #     text = response.text.strip()
# #     print("üîÅ Gemini Response:", text)
# #     return text


# # def is_structured_json(text):
# #     try:
# #         match = re.search(r"\{[\s\S]*\}", text)
# #         if match:
# #             parsed = json.loads(match.group())
# #             if "‡§∂‡§ø‡§ï‡§æ‡§Ø‡§§" in parsed:
# #                 return parsed
# #     except json.JSONDecodeError:
# #         return None
# #     return None

# # import google.generativeai as genai
# # import json
# # import re

# # # ‚úÖ Use your API key directly
# # #AIzaSyDQ8agyfEwaijZ0VpByd1I71cnzIuKuXvc
# # #AIzaSyDUClq0SgIcsHqjtQzpHpr7FcPkyVQMWOM
# # API_KEY = "AIzaSyDQ8agyfEwaijZ0VpByd1I71cnzIuKuXvc"
# # genai.configure(api_key=API_KEY)

# # # Gemini model
# # model = genai.GenerativeModel(model_name="models/gemini-1.5-flash")

# # # ‚úÖ Only ONE correct version of this function
# # def ask_gemini_followup_or_result(conversation_log):
# #     prompt = f"""
# #     ‡§§‡•Å‡§Æ ‡§è‡§ï ‡§®‡§æ‡§ó‡§∞‡§ø‡§ï ‡§∏‡§π‡§æ‡§Ø‡§§‡§æ ‡§¨‡•ã‡§ü ‡§π‡•ã ‡§ú‡•ã ‡§Ø‡•Ç‡§ú‡§º‡§∞ ‡§∏‡•á ‡§ï‡•ç‡§∞‡§Æ‡§∂‡§É ‡§è‡§ï-‡§è‡§ï ‡§ï‡§∞ ‡§ï‡•á ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§≤‡•á‡§§‡•Ä ‡§π‡•ã‡•§

# #     ‡§Ö‡§≠‡•Ä ‡§§‡§ï ‡§ï‡•Ä ‡§¨‡§æ‡§§‡§ö‡•Ä‡§§:
# #     {conversation_log}

# #     ‡§Ö‡§¨ ‡§ú‡•ã ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§®‡§π‡•Ä‡§Ç ‡§≤‡•Ä ‡§ó‡§à ‡§π‡•à, ‡§ï‡•á‡§µ‡§≤ ‡§â‡§∏‡§ï‡§æ ‡§Ö‡§ó‡§≤‡§æ ‡§∏‡§µ‡§æ‡§≤ ‡§™‡•Ç‡§õ‡•ã‡•§
# #     ‡§Ø‡§¶‡§ø ‡§∏‡§≠‡•Ä ‡§ú‡§º‡§∞‡•Ç‡§∞‡•Ä ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§Æ‡§ø‡§≤ ‡§ö‡•Å‡§ï‡•Ä ‡§π‡•à, ‡§§‡•ã ‡§ï‡•É‡§™‡§Ø‡§æ ‡§è‡§ï JSON ‡§â‡§§‡•ç‡§§‡§∞ ‡§¶‡•ã ‡§ú‡§ø‡§∏‡§Æ‡•á‡§Ç ‡§Ø‡•á ‡§´‡§º‡•Ä‡§≤‡•ç‡§° ‡§π‡•ã‡§Ç:
# #     {{
# #       "‡§∂‡§ø‡§ï‡§æ‡§Ø‡§§": "...",
# #       "‡§∏‡•ç‡§•‡§æ‡§®": "...",
# #       "‡§∂‡§ø‡§ï‡§æ‡§Ø‡§§‡§ï‡§∞‡•ç‡§§‡§æ ‡§ï‡§æ ‡§®‡§æ‡§Æ": "...",
# #       "‡§Æ‡•ã‡§¨‡§æ‡§á‡§≤ ‡§®‡§Ç‡§¨‡§∞": "...",
# #       "‡§¨‡•ã‡§≤‡§®‡•á_‡§≤‡§æ‡§Ø‡§ï_‡§∏‡§æ‡§∞‡§æ‡§Ç‡§∂": "...",
# #       "‡§Ö‡§Ç‡§§‡§ø‡§Æ_‡§ò‡•ã‡§∑‡§£‡§æ": "...",
# #       "‡§µ‡§ø‡§≠‡§æ‡§ó": "..."
# #     }}
# #     """
# #     response = model.generate_content(prompt)
# #     text = response.text.strip()
# #     print("ü§ñ Gemini Response:", text)  # Optional for debugging
# #     return text

# # # ‚úÖ JSON detection logic
# # def is_structured_json(text):
# #     try:
# #         match = re.search(r"\{[\s\S]*\}", text)
# #         if match:
# #             parsed = json.loads(match.group())
# #             if "‡§∂‡§ø‡§ï‡§æ‡§Ø‡§§" in parsed:
# #                 return parsed
# #     except json.JSONDecodeError:
# #         return None
# #     return None



# import google.generativeai as genai
# import json
# import re
# import time
# from datetime import datetime

# # ‚úÖ Use your API key directly
# API_KEY = "AIzaSyDQ8agyfEwaijZ0VpByd1I71cnzIuKuXvc"
# genai.configure(api_key=API_KEY)

# # Gemini model
# model = genai.GenerativeModel(model_name="models/gemini-1.5-flash")

# # Cache for reducing API calls
# response_cache = {}


# def generate_complaint_id():
#     """Generate unique complaint ID"""
#     timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
#     return f"CMP-{timestamp}"


# def get_department_from_complaint(complaint_text):
#     """Simple rule-based department detection to save API calls"""
#     complaint_lower = complaint_text.lower()
    
#     # Road related
#     if any(word in complaint_lower for word in ["‡§∏‡§°‡§º‡§ï", "‡§ó‡§°‡•ç‡§¢‡§æ", "road", "street", "pothole"]):
#         return "‡§∏‡§°‡§º‡§ï ‡§µ‡§ø‡§≠‡§æ‡§ó"
    
#     # Water related
#     elif any(word in complaint_lower for word in ["‡§™‡§æ‡§®‡•Ä", "‡§®‡§≤", "water", "tap", "pipe"]):
#         return "‡§ú‡§≤ ‡§µ‡§ø‡§≠‡§æ‡§ó"
    
#     # Electricity related
#     elif any(word in complaint_lower for word in ["‡§¨‡§ø‡§ú‡§≤‡•Ä", "light", "electricity", "power"]):
#         return "‡§µ‡§ø‡§¶‡•ç‡§Ø‡•Å‡§§ ‡§µ‡§ø‡§≠‡§æ‡§ó"
    
#     # Sanitation related
#     elif any(word in complaint_lower for word in ["‡§ï‡•Ç‡§°‡§º‡§æ", "‡§ó‡§Ç‡§¶‡§ó‡•Ä", "garbage", "waste", "cleaning"]):
#         return "‡§∏‡•ç‡§µ‡§ö‡•ç‡§õ‡§§‡§æ ‡§µ‡§ø‡§≠‡§æ‡§ó"
    
#     # Health related
#     elif any(word in complaint_lower for word in ["‡§Ö‡§∏‡•ç‡§™‡§§‡§æ‡§≤", "doctor", "health", "medical"]):
#         return "‡§∏‡•ç‡§µ‡§æ‡§∏‡•ç‡§•‡•ç‡§Ø ‡§µ‡§ø‡§≠‡§æ‡§ó"
    
#     # Education related
#     elif any(word in complaint_lower for word in ["‡§∏‡•ç‡§ï‡•Ç‡§≤", "school", "education", "teacher"]):
#         return "‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ ‡§µ‡§ø‡§≠‡§æ‡§ó"
    
#     # Default
#     else:
#         return "‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§™‡•ç‡§∞‡§∂‡§æ‡§∏‡§®"


# def ask_gemini_followup_or_result(conversation_log):
#     """
#     Simplified version - only use AI for complex cases
#     This saves API quota for your free account
#     """
    
#     # Check cache first
#     cache_key = hash(conversation_log)
#     if cache_key in response_cache:
#         return response_cache[cache_key]
    
#     # Simple prompt to save tokens
#     prompt = f"""
#     ‡§¨‡§æ‡§§‡§ö‡•Ä‡§§: {conversation_log}
    
#     ‡§Ö‡§ó‡§≤‡§æ ‡§∏‡§µ‡§æ‡§≤ ‡§™‡•Ç‡§õ‡•ã ‡§Ø‡§æ JSON ‡§¨‡§®‡§æ‡§ì:
#     {{
#       "‡§∂‡§ø‡§ï‡§æ‡§Ø‡§§": "...",
#       "‡§∏‡•ç‡§•‡§æ‡§®": "...",
#       "‡§∂‡§ø‡§ï‡§æ‡§Ø‡§§‡§ï‡§∞‡•ç‡§§‡§æ ‡§ï‡§æ ‡§®‡§æ‡§Æ": "...",
#       "‡§Æ‡•ã‡§¨‡§æ‡§á‡§≤ ‡§®‡§Ç‡§¨‡§∞": "...",
#       "‡§µ‡§ø‡§≠‡§æ‡§ó": "..."
#     }}
#     """
    
#     try:
#         response = model.generate_content(prompt)
#         text = response.text.strip()
        
#         # Cache the response
#         response_cache[cache_key] = text
        
#         print("ü§ñ Gemini Response:", text[:100] + "..." if len(text) > 100 else text)
#         return text
        
#     except Exception as e:
#         print(f"‚ùå Gemini API Error: {e}")
#         # Fallback response
#         return "‡§ï‡•É‡§™‡§Ø‡§æ ‡§Ö‡§™‡§®‡•Ä ‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç ‡§î‡§∞ ‡§¨‡§§‡§æ‡§è‡§Ç‡•§"


# def ask_smart_followup(current_data, missing_fields):
#     """
#     Smart followup questions without using API
#     This saves your quota significantly
#     """
    
#     if "‡§∂‡§ø‡§ï‡§æ‡§Ø‡§§" in missing_fields:
#         return "‡§Ü‡§™‡§ï‡•Ä ‡§Æ‡•Å‡§ñ‡•ç‡§Ø ‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à? ‡§ï‡•É‡§™‡§Ø‡§æ ‡§µ‡§ø‡§∏‡•ç‡§§‡§æ‡§∞ ‡§∏‡•á ‡§¨‡§§‡§æ‡§è‡§Ç‡•§"
    
#     elif "‡§∏‡•ç‡§•‡§æ‡§®" in missing_fields:
#         return "‡§Ø‡§π ‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ ‡§ï‡§π‡§æ‡§Å ‡§π‡•ã ‡§∞‡§π‡•Ä ‡§π‡•à? ‡§ï‡•É‡§™‡§Ø‡§æ ‡§™‡•Ç‡§∞‡§æ ‡§™‡§§‡§æ ‡§¨‡§§‡§æ‡§è‡§Ç‡•§"
    
#     elif "‡§∂‡§ø‡§ï‡§æ‡§Ø‡§§‡§ï‡§∞‡•ç‡§§‡§æ ‡§ï‡§æ ‡§®‡§æ‡§Æ" in missing_fields:
#         return "‡§Ü‡§™‡§ï‡§æ ‡§®‡§æ‡§Æ ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à?"
    
#     elif "‡§Æ‡•ã‡§¨‡§æ‡§á‡§≤ ‡§®‡§Ç‡§¨‡§∞" in missing_fields:
#         return "‡§ï‡•É‡§™‡§Ø‡§æ ‡§Ö‡§™‡§®‡§æ ‡§Æ‡•ã‡§¨‡§æ‡§á‡§≤ ‡§®‡§Ç‡§¨‡§∞ ‡§¨‡§§‡§æ‡§è‡§Ç ‡§§‡§æ‡§ï‡§ø ‡§π‡§Æ ‡§Ü‡§™‡§∏‡•á ‡§∏‡§Ç‡§™‡§∞‡•ç‡§ï ‡§ï‡§∞ ‡§∏‡§ï‡•á‡§Ç‡•§"
    
#     else:
#         # All info collected, create final JSON
#         department = get_department_from_complaint(current_data.get("‡§∂‡§ø‡§ï‡§æ‡§Ø‡§§", ""))
        
#         final_data = {
#             "‡§∂‡§ø‡§ï‡§æ‡§Ø‡§§": current_data.get("‡§∂‡§ø‡§ï‡§æ‡§Ø‡§§", ""),
#             "‡§∏‡•ç‡§•‡§æ‡§®": current_data.get("‡§∏‡•ç‡§•‡§æ‡§®", ""),
#             "‡§∂‡§ø‡§ï‡§æ‡§Ø‡§§‡§ï‡§∞‡•ç‡§§‡§æ ‡§ï‡§æ ‡§®‡§æ‡§Æ": current_data.get("‡§∂‡§ø‡§ï‡§æ‡§Ø‡§§‡§ï‡§∞‡•ç‡§§‡§æ ‡§ï‡§æ ‡§®‡§æ‡§Æ", ""),
#             "‡§Æ‡•ã‡§¨‡§æ‡§á‡§≤ ‡§®‡§Ç‡§¨‡§∞": current_data.get("‡§Æ‡•ã‡§¨‡§æ‡§á‡§≤ ‡§®‡§Ç‡§¨‡§∞", ""),
#             "‡§µ‡§ø‡§≠‡§æ‡§ó": department,
#             "complaint_id": generate_complaint_id(),
#             "‡§¨‡•ã‡§≤‡§®‡•á_‡§≤‡§æ‡§Ø‡§ï_‡§∏‡§æ‡§∞‡§æ‡§Ç‡§∂": f"‡§Ü‡§™‡§ï‡•Ä ‡§∂‡§ø‡§ï‡§æ‡§Ø‡§§ {department} ‡§ï‡•ã ‡§≠‡•á‡§ú‡•Ä ‡§ú‡§æ ‡§∞‡§π‡•Ä ‡§π‡•à‡•§",
#             "‡§Ö‡§Ç‡§§‡§ø‡§Æ_‡§ò‡•ã‡§∑‡§£‡§æ": "‡§Ü‡§™‡§ï‡•Ä ‡§∂‡§ø‡§ï‡§æ‡§Ø‡§§ ‡§∏‡§´‡§≤‡§§‡§æ‡§™‡•Ç‡§∞‡•ç‡§µ‡§ï ‡§¶‡§∞‡•ç‡§ú ‡§π‡•ã ‡§ó‡§à ‡§π‡•à‡•§ ‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶!"
#         }
        
#         return json.dumps(final_data, ensure_ascii=False)


# def is_structured_json(text):
#     """Enhanced JSON detection logic"""
#     try:
#         # Look for JSON pattern
#         match = re.search(r"\{[\s\S]*\}", text)
#         if match:
#             json_str = match.group()
#             parsed = json.loads(json_str)
            
#             # Check if it's a complaint JSON
#             if "‡§∂‡§ø‡§ï‡§æ‡§Ø‡§§" in parsed or "complaint" in str(parsed).lower():
#                 return parsed
                
#         # Try to parse the entire text as JSON
#         parsed = json.loads(text)
#         if isinstance(parsed, dict) and "‡§∂‡§ø‡§ï‡§æ‡§Ø‡§§" in parsed:
#             return parsed
            
#     except (json.JSONDecodeError, AttributeError):
#         pass
    
#     return None


# def validate_phone_number(phone):
#     """Simple phone number validation"""
#     import re
#     # Remove spaces and common characters
#     phone = re.sub(r'[^\d]', '', phone)
    
#     # Check if it's a valid Indian mobile number
#     if len(phone) == 10 and phone.startswith(('6', '7', '8', '9')):
#         return True
#     elif len(phone) == 13 and phone.startswith('91'):
#         return True
    
#     return False


# def create_final_complaint(complaint_data):
#     """Create final complaint JSON with all validations"""
    
#     # Validate required fields
#     required_fields = ["‡§∂‡§ø‡§ï‡§æ‡§Ø‡§§", "‡§∏‡•ç‡§•‡§æ‡§®", "‡§∂‡§ø‡§ï‡§æ‡§Ø‡§§‡§ï‡§∞‡•ç‡§§‡§æ ‡§ï‡§æ ‡§®‡§æ‡§Æ", "‡§Æ‡•ã‡§¨‡§æ‡§á‡§≤ ‡§®‡§Ç‡§¨‡§∞"]
    
#     for field in required_fields:
#         if not complaint_data.get(field, "").strip():
#             return None
    
#     # Validate phone number
#     if not validate_phone_number(complaint_data["‡§Æ‡•ã‡§¨‡§æ‡§á‡§≤ ‡§®‡§Ç‡§¨‡§∞"]):
#         return None
    
#     # Get department
#     department = get_department_from_complaint(complaint_data["‡§∂‡§ø‡§ï‡§æ‡§Ø‡§§"])
    
#     # Create final structure
#     final_complaint = {
#         "complaint_id": generate_complaint_id(),
#         "‡§∂‡§ø‡§ï‡§æ‡§Ø‡§§": complaint_data["‡§∂‡§ø‡§ï‡§æ‡§Ø‡§§"].strip(),
#         "‡§∏‡•ç‡§•‡§æ‡§®": complaint_data["‡§∏‡•ç‡§•‡§æ‡§®"].strip(),
#         "‡§∂‡§ø‡§ï‡§æ‡§Ø‡§§‡§ï‡§∞‡•ç‡§§‡§æ ‡§ï‡§æ ‡§®‡§æ‡§Æ": complaint_data["‡§∂‡§ø‡§ï‡§æ‡§Ø‡§§‡§ï‡§∞‡•ç‡§§‡§æ ‡§ï‡§æ ‡§®‡§æ‡§Æ"].strip(),
#         "‡§Æ‡•ã‡§¨‡§æ‡§á‡§≤ ‡§®‡§Ç‡§¨‡§∞": complaint_data["‡§Æ‡•ã‡§¨‡§æ‡§á‡§≤ ‡§®‡§Ç‡§¨‡§∞"].strip(),
#         "‡§µ‡§ø‡§≠‡§æ‡§ó": department,
#         "timestamp": datetime.now().isoformat(),
#         "status": "‡§¶‡§∞‡•ç‡§ú",
#         "priority": "medium",
#         "‡§¨‡•ã‡§≤‡§®‡•á_‡§≤‡§æ‡§Ø‡§ï_‡§∏‡§æ‡§∞‡§æ‡§Ç‡§∂": f"‡§Ü‡§™‡§ï‡•Ä ‡§∂‡§ø‡§ï‡§æ‡§Ø‡§§ ‡§∏‡§Ç‡§ñ‡•ç‡§Ø‡§æ {generate_complaint_id()[:8]} ‡§¶‡§∞‡•ç‡§ú ‡§π‡•ã ‡§ó‡§à ‡§π‡•à‡•§ ‡§Ø‡§π {department} ‡§ï‡•ã ‡§≠‡•á‡§ú‡•Ä ‡§ú‡§æ ‡§∞‡§π‡•Ä ‡§π‡•à‡•§",
#         "‡§Ö‡§Ç‡§§‡§ø‡§Æ_‡§ò‡•ã‡§∑‡§£‡§æ": "‡§Ü‡§™‡§ï‡•Ä ‡§∂‡§ø‡§ï‡§æ‡§Ø‡§§ ‡§∏‡§´‡§≤‡§§‡§æ‡§™‡•Ç‡§∞‡•ç‡§µ‡§ï ‡§¶‡§∞‡•ç‡§ú ‡§π‡•ã ‡§ó‡§à ‡§π‡•à‡•§ ‡§ú‡§≤‡•ç‡§¶ ‡§π‡•Ä ‡§ï‡§æ‡§∞‡•ç‡§∞‡§µ‡§æ‡§à ‡§ï‡•Ä ‡§ú‡§æ‡§è‡§ó‡•Ä‡•§ ‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶!"
#     }
    
#     return final_complaint


import google.generativeai as genai
import json
import re
import time
from datetime import datetime

# Configure Gemini API
API_KEY = "AIzaSyDQ8agyfEwaijZ0VpByd1I71cnzIuKuXvgfc" 
#AIzaSyDQ8agyfEwaijZ0VpByd1I71cnzIuKuXvc
genai.configure(api_key=API_KEY)

# Initialize Gemini model
model = genai.GenerativeModel(
    model_name="models/gemini-1.5-pro",
    generation_config={
        "temperature": 0.7,
        "top_p": 0.8,
        "top_k": 40,
        "max_output_tokens": 1000,
    }
)

# Cache for reducing API calls
response_cache = {}



def ask_gemini_for_followup(conversation_text, initial_complaint):
    """Ask Gemini to generate intelligent follow-up questions"""
    
    # Create cache key
    cache_key = hash(conversation_text + initial_complaint)
    if cache_key in response_cache:
        return response_cache[cache_key]
    
    prompt = f"""
‡§§‡•Å‡§Æ ‡§è‡§ï ‡§∏‡§∞‡§ï‡§æ‡§∞‡•Ä ‡§∂‡§ø‡§ï‡§æ‡§Ø‡§§ ‡§∏‡§π‡§æ‡§Ø‡§ï ‡§π‡•ã‡•§ ‡§®‡§æ‡§ó‡§∞‡§ø‡§ï ‡§ï‡•Ä ‡§∂‡§ø‡§ï‡§æ‡§Ø‡§§ ‡§ï‡•ã ‡§∏‡§Æ‡§ù‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§¨‡•Å‡§¶‡•ç‡§ß‡§ø‡§Æ‡§æ‡§®‡•Ä ‡§∏‡•á ‡§Ö‡§ó‡§≤‡§æ ‡§∏‡§µ‡§æ‡§≤ ‡§™‡•Ç‡§õ‡•ã‡•§

‡§Æ‡•Å‡§ñ‡•ç‡§Ø ‡§∂‡§ø‡§ï‡§æ‡§Ø‡§§: {initial_complaint}

‡§Ö‡§¨ ‡§§‡§ï ‡§ï‡•Ä ‡§¨‡§æ‡§§‡§ö‡•Ä‡§§:
{conversation_text}

‡§®‡§ø‡§∞‡•ç‡§¶‡•á‡§∂:
1. ‡§Ö‡§ó‡§∞ ‡§ú‡§∞‡•Ç‡§∞‡•Ä ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§Æ‡§ø‡§≤ ‡§ó‡§à ‡§π‡•à ‡§§‡•ã "COMPLETE" ‡§≤‡§ø‡§ñ‡•ã
2. ‡§Ö‡§ó‡§∞ ‡§î‡§∞ ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§ö‡§æ‡§π‡§ø‡§è ‡§§‡•ã ‡§è‡§ï ‡§∏‡•ç‡§™‡§∑‡•ç‡§ü, ‡§µ‡•ç‡§Ø‡§æ‡§µ‡§π‡§æ‡§∞‡§ø‡§ï ‡§∏‡§µ‡§æ‡§≤ ‡§™‡•Ç‡§õ‡•ã
3. ‡§∏‡§ø‡§∞‡•ç‡§´ ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§ú‡§µ‡§æ‡§¨ ‡§¶‡•ã
4. ‡§¨‡§π‡•Å‡§§ ‡§≤‡§Ç‡§¨‡§æ ‡§∏‡§µ‡§æ‡§≤ ‡§® ‡§™‡•Ç‡§õ‡•ã

‡§Ö‡§ó‡§≤‡§æ ‡§∏‡§µ‡§æ‡§≤:"""

    try:
        response = model.generate_content(prompt)
        follow_up = response.text.strip()
        
        # Cache the response
        response_cache[cache_key] = follow_up
        
        print(f"ü§ñ Gemini Follow-up: {follow_up}")
        return follow_up
        
    except Exception as e:
        print(f"‚ùå Gemini Follow-up Error: {e}")
        # Fallback questions
        fallback_questions = [
            
            "‡§Ø‡§π ‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ ‡§ï‡§ø‡§§‡§®‡•á ‡§¶‡§ø‡§®‡•ã‡§Ç ‡§∏‡•á ‡§ö‡§≤ ‡§∞‡§π‡•Ä ‡§π‡•à?",
            "‡§á‡§∏‡§∏‡•á ‡§ï‡§ø‡§§‡§®‡•á ‡§≤‡•ã‡§ó ‡§™‡•ç‡§∞‡§≠‡§æ‡§µ‡§ø‡§§ ‡§π‡•à‡§Ç?",
            "‡§ï‡•ç‡§Ø‡§æ ‡§Ü‡§™‡§®‡•á ‡§™‡§π‡§≤‡•á ‡§á‡§∏ ‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ï‡§ø‡§∏‡•Ä ‡§∏‡•á ‡§∏‡§Ç‡§™‡§∞‡•ç‡§ï ‡§ï‡§ø‡§Ø‡§æ ‡§π‡•à?",

        ]
        
        import random
        return random.choice(fallback_questions)


def analyze_complaint_with_gemini(conversation_text):
    """Use Gemini to analyze the full conversation and extract structured data"""
    
    prompt = f"""
‡§§‡•Å‡§Æ ‡§è‡§ï ‡§∏‡§∞‡§ï‡§æ‡§∞‡•Ä ‡§∂‡§ø‡§ï‡§æ‡§Ø‡§§ ‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§ï ‡§π‡•ã‡•§ ‡§á‡§∏ ‡§¨‡§æ‡§§‡§ö‡•Ä‡§§ ‡§∏‡•á ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§®‡§ø‡§ï‡§æ‡§≤‡§ï‡§∞ JSON format ‡§Æ‡•á‡§Ç ‡§¶‡•ã‡•§

‡§¨‡§æ‡§§‡§ö‡•Ä‡§§:
{conversation_text}

‡§®‡§ø‡§Æ‡•ç‡§®‡§≤‡§ø‡§ñ‡§ø‡§§ JSON format ‡§Æ‡•á‡§Ç ‡§ú‡§µ‡§æ‡§¨ ‡§¶‡•ã:
{{
    "department": "‡§µ‡§ø‡§≠‡§æ‡§ó ‡§ï‡§æ ‡§®‡§æ‡§Æ",
    "priority": "urgency level",
    "description": "‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ ‡§ï‡§æ ‡§™‡•Ç‡§∞‡§æ ‡§µ‡§ø‡§µ‡§∞‡§£ "
}}

‡§µ‡§ø‡§≠‡§æ‡§ó ‡§ï‡•á ‡§µ‡§ø‡§ï‡§≤‡•ç‡§™:
- ‡§ú‡§≤ ‡§µ‡§ø‡§≠‡§æ‡§ó (‡§™‡§æ‡§®‡•Ä, ‡§®‡§≤, ‡§™‡§æ‡§á‡§™ ‡§ï‡•Ä ‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ)
- ‡§µ‡§ø‡§¶‡•ç‡§Ø‡•Å‡§§ ‡§µ‡§ø‡§≠‡§æ‡§ó (‡§¨‡§ø‡§ú‡§≤‡•Ä, ‡§≤‡§æ‡§á‡§ü ‡§ï‡•Ä ‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ) 
- ‡§∏‡§°‡§º‡§ï ‡§µ‡§ø‡§≠‡§æ‡§ó (‡§∏‡§°‡§º‡§ï, ‡§ó‡§°‡•ç‡§¢‡•á ‡§ï‡•Ä ‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ)
- ‡§∏‡•ç‡§µ‡§ö‡•ç‡§õ‡§§‡§æ ‡§µ‡§ø‡§≠‡§æ‡§ó (‡§ï‡•Ç‡§°‡§º‡§æ, ‡§ó‡§Ç‡§¶‡§ó‡•Ä ‡§ï‡•Ä ‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ)
- ‡§∏‡•ç‡§µ‡§æ‡§∏‡•ç‡§•‡•ç‡§Ø ‡§µ‡§ø‡§≠‡§æ‡§ó (‡§Ö‡§∏‡•ç‡§™‡§§‡§æ‡§≤, ‡§°‡•â‡§ï‡•ç‡§ü‡§∞ ‡§ï‡•Ä ‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ)
- ‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ ‡§µ‡§ø‡§≠‡§æ‡§ó (‡§∏‡•ç‡§ï‡•Ç‡§≤, ‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ ‡§ï‡•Ä ‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ)
- ‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§™‡•ç‡§∞‡§∂‡§æ‡§∏‡§® (‡§Ö‡§®‡•ç‡§Ø ‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ‡§è‡§Ç)

Priority ‡§ï‡•á ‡§µ‡§ø‡§ï‡§≤‡•ç‡§™: "low", "medium", "high"

‡§∏‡§ø‡§∞‡•ç‡§´ JSON ‡§Æ‡•á‡§Ç ‡§ú‡§µ‡§æ‡§¨ ‡§¶‡•ã, ‡§ï‡•Å‡§õ ‡§î‡§∞ ‡§® ‡§≤‡§ø‡§ñ‡•ã‡•§"""

    try:
        response = model.generate_content(prompt)
        response_text = response.text.strip()
        
        print(f"ü§ñ Gemini Analysis Response: {response_text}")
        
        # Extract JSON from response
        json_match = re.search(r'\{[\s\S]*\}', response_text)
        if json_match:
            json_str = json_match.group()
            try:
                analysis_data = json.loads(json_str)
                print(f"‚úÖ Parsed Analysis: {analysis_data}")
                return analysis_data
            except json.JSONDecodeError as e:
                print(f"‚ùå JSON Parse Error: {e}")
                return None
        else:
            print("‚ùå No JSON found in response")
            return None
            
    except Exception as e:
        print(f"‚ùå Gemini Analysis Error: {e}")
        return None


def is_conversation_complete(conversation_text, current_data):
    """Check if enough information has been gathered"""
    
    prompt = f"""
‡§á‡§∏ ‡§¨‡§æ‡§§‡§ö‡•Ä‡§§ ‡§ï‡•ã ‡§¶‡•á‡§ñ‡§ï‡§∞ ‡§¨‡§§‡§æ‡§ì ‡§ï‡§ø ‡§ï‡•ç‡§Ø‡§æ ‡§∂‡§ø‡§ï‡§æ‡§Ø‡§§ ‡§¶‡§∞‡•ç‡§ú ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§™‡§∞‡•ç‡§Ø‡§æ‡§™‡•ç‡§§ ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§Æ‡§ø‡§≤ ‡§ó‡§à ‡§π‡•à‡•§

‡§¨‡§æ‡§§‡§ö‡•Ä‡§§:
{conversation_text}

‡§µ‡§∞‡•ç‡§§‡§Æ‡§æ‡§® ‡§°‡•á‡§ü‡§æ:
‡§∂‡§ø‡§ï‡§æ‡§Ø‡§§: {current_data.get('complaint', '')}

‡§∏‡§ø‡§∞‡•ç‡§´ "YES" ‡§Ø‡§æ "NO" ‡§Æ‡•á‡§Ç ‡§ú‡§µ‡§æ‡§¨ ‡§¶‡•ã‡•§
YES - ‡§Ö‡§ó‡§∞ ‡§Æ‡•Å‡§ñ‡•ç‡§Ø ‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ ‡§∏‡•ç‡§™‡§∑‡•ç‡§ü ‡§π‡•à ‡§î‡§∞ ‡§¨‡•Å‡§®‡§ø‡§Ø‡§æ‡§¶‡•Ä ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§Æ‡§ø‡§≤ ‡§ó‡§à ‡§π‡•à
NO - ‡§Ö‡§ó‡§∞ ‡§î‡§∞ ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§ï‡•Ä ‡§ú‡§∞‡•Ç‡§∞‡§§ ‡§π‡•à"""

    try:
        response = model.generate_content(prompt)
        result = response.text.strip().upper()
        
        print(f"ü§ñ Conversation Complete Check: {result}")
        return result == "YES"
        
    except Exception as e:
        print(f"‚ùå Completion Check Error: {e}")
        # Fallback logic - check if we have basic info
        return len(conversation_text) > 200 and current_data.get('complaint', '')


def create_conversation_summary(conversation_history):
    """Create a summary of the conversation for the admin dashboard"""
    
    if not conversation_history:
        return "‡§ï‡•ã‡§à ‡§¨‡§æ‡§§‡§ö‡•Ä‡§§ ‡§∞‡§ø‡§ï‡•â‡§∞‡•ç‡§° ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤‡•Ä‡•§"
    
    # Format conversation for summary
    conversation_text = "\n".join([
        f"{entry['speaker']}: {entry['message']}" 
        for entry in conversation_history
    ])
    
    prompt = f"""
‡§á‡§∏ ‡§¨‡§æ‡§§‡§ö‡•Ä‡§§ ‡§ï‡§æ ‡§è‡§ï ‡§∏‡§Ç‡§ï‡•ç‡§∑‡§ø‡§™‡•ç‡§§ ‡§∏‡§æ‡§∞‡§æ‡§Ç‡§∂ ‡§§‡•à‡§Ø‡§æ‡§∞ ‡§ï‡§∞‡•ã ‡§ú‡•ã admin dashboard ‡§Æ‡•á‡§Ç ‡§¶‡§ø‡§ñ‡§æ‡§Ø‡§æ ‡§ú‡§æ ‡§∏‡§ï‡•á‡•§

‡§¨‡§æ‡§§‡§ö‡•Ä‡§§:
{conversation_text}

‡§∏‡§æ‡§∞‡§æ‡§Ç‡§∂ ‡§Æ‡•á‡§Ç ‡§∂‡§æ‡§Æ‡§ø‡§≤ ‡§ï‡§∞‡•ã:
1. ‡§Æ‡•Å‡§ñ‡•ç‡§Ø ‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à
2. ‡§ï‡•ã‡§à ‡§Æ‡§π‡§§‡•ç‡§µ‡§™‡•Ç‡§∞‡•ç‡§£ ‡§µ‡§ø‡§µ‡§∞‡§£
3. ‡§â‡§™‡§Ø‡•ã‡§ó‡§ï‡§∞‡•ç‡§§‡§æ ‡§ï‡•Ä ‡§™‡•ç‡§∞‡§§‡§ø‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ

50 ‡§∂‡§¨‡•ç‡§¶‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§≤‡§ø‡§ñ‡•ã‡•§"""

    try:
        response = model.generate_content(prompt)
        summary = response.text.strip()
        
        print(f"üìù Conversation Summary: {summary[:100]}...")
        return summary
        
    except Exception as e:
        print(f"‚ùå Summary Error: {e}")
        # Fallback summary
        user_messages = [entry['message'] for entry in conversation_history if entry['speaker'] == 'User']
        if user_messages:
            return f"‡§â‡§™‡§Ø‡•ã‡§ó‡§ï‡§∞‡•ç‡§§‡§æ ‡§ï‡•Ä ‡§Æ‡•Å‡§ñ‡•ç‡§Ø ‡§∂‡§ø‡§ï‡§æ‡§Ø‡§§: {user_messages[0]}\n\n‡§ï‡•Å‡§≤ ‡§∏‡§Ç‡§¶‡•á‡§∂: {len(conversation_history)}"
        else:
            return "‡§µ‡•â‡§á‡§∏ ‡§∂‡§ø‡§ï‡§æ‡§Ø‡§§ - ‡§¨‡§æ‡§§‡§ö‡•Ä‡§§ ‡§ï‡§æ ‡§∏‡§æ‡§∞‡§æ‡§Ç‡§∂ ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§®‡§π‡•Ä‡§Ç‡•§"


def get_department_and_priority(complaint_text):
    """Get department and priority using Gemini (fallback function)"""
    
    prompt = f"""
‡§á‡§∏ ‡§∂‡§ø‡§ï‡§æ‡§Ø‡§§ ‡§ï‡•ã ‡§¶‡•á‡§ñ‡§ï‡§∞ ‡§∏‡§π‡•Ä ‡§µ‡§ø‡§≠‡§æ‡§ó ‡§î‡§∞ priority ‡§¨‡§§‡§æ‡§ì‡•§

‡§∂‡§ø‡§ï‡§æ‡§Ø‡§§: {complaint_text}

JSON format ‡§Æ‡•á‡§Ç ‡§ú‡§µ‡§æ‡§¨ ‡§¶‡•ã:
{{
    "department": "‡§µ‡§ø‡§≠‡§æ‡§ó ‡§ï‡§æ ‡§®‡§æ‡§Æ",
    "priority": "low/medium/high"
}}

‡§µ‡§ø‡§≠‡§æ‡§ó ‡§ï‡•á ‡§µ‡§ø‡§ï‡§≤‡•ç‡§™:
- ‡§ú‡§≤ ‡§µ‡§ø‡§≠‡§æ‡§ó
- ‡§µ‡§ø‡§¶‡•ç‡§Ø‡•Å‡§§ ‡§µ‡§ø‡§≠‡§æ‡§ó  
- ‡§∏‡§°‡§º‡§ï ‡§µ‡§ø‡§≠‡§æ‡§ó
- ‡§∏‡•ç‡§µ‡§ö‡•ç‡§õ‡§§‡§æ ‡§µ‡§ø‡§≠‡§æ‡§ó
- ‡§∏‡•ç‡§µ‡§æ‡§∏‡•ç‡§•‡•ç‡§Ø ‡§µ‡§ø‡§≠‡§æ‡§ó
- ‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ ‡§µ‡§ø‡§≠‡§æ‡§ó
- ‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§™‡•ç‡§∞‡§∂‡§æ‡§∏‡§®"""

    try:
        response = model.generate_content(prompt)
        json_match = re.search(r'\{[\s\S]*\}', response.text)
        
        if json_match:
            return json.loads(json_match.group())
        else:
            return {"department": "‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§™‡•ç‡§∞‡§∂‡§æ‡§∏‡§®", "priority": "medium"}
            
    except Exception as e:
        print(f"‚ùå Department/Priority Error: {e}")
        return {"department": "‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§™‡•ç‡§∞‡§∂‡§æ‡§∏‡§®", "priority": "medium"}


def validate_phone_number(phone_str):
    """Enhanced phone number validation"""
    if not phone_str:
        return False
    
    # Remove all non-digit characters
    phone = re.sub(r'[^\d]', '', str(phone_str))
    
    # Check various Indian phone number patterns
    if len(phone) == 10 and phone[0] in '6789':
        return True
    elif len(phone) == 11 and phone.startswith('0') and phone[1] in '6789':
        return True
    elif len(phone) == 12 and phone.startswith('91') and phone[2] in '6789':
        return True
    elif len(phone) == 13 and phone.startswith('+91') and phone[3] in '6789':
        return True
    
    return False


def clean_phone_number(phone_str):
    """Clean and format phone number"""
    if not phone_str:
        return ""
    
    phone = re.sub(r'[^\d]', '', str(phone_str))
    
    if len(phone) == 10:
        return phone
    elif len(phone) == 11 and phone.startswith('0'):
        return phone[1:]
    elif len(phone) == 12 and phone.startswith('91'):
        return phone[2:]
    elif len(phone) == 13 and phone.startswith('+91'):
        return phone[3:]
    
    return phone


# Test function for debugging
def test_gemini_connection():
    """Test if Gemini API is working"""
    try:
        test_prompt = "‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç 'Hello' ‡§≤‡§ø‡§ñ‡•ã‡•§"
        response = model.generate_content(test_prompt)
        print(f"‚úÖ Gemini Test Success: {response.text}")
        return True
    except Exception as e:
        print(f"‚ùå Gemini Test Failed: {e}")
        return False


if __name__ == "__main__":
    # Test the connection
    print("üß™ Testing Gemini AI connection...")
    test_gemini_connection()
    
    # Test analysis function
    sample_conversation = """
‡§â‡§™‡§Ø‡•ã‡§ó‡§ï‡§∞‡•ç‡§§‡§æ: ‡§Æ‡•á‡§∞‡•á ‡§á‡§≤‡§æ‡§ï‡•á ‡§Æ‡•á‡§Ç ‡§™‡§æ‡§®‡•Ä ‡§®‡§π‡•Ä‡§Ç ‡§Ü ‡§∞‡§π‡§æ ‡§™‡§ø‡§õ‡§≤‡•á 5 ‡§¶‡§ø‡§®‡•ã‡§Ç ‡§∏‡•á
‡§¨‡•â‡§ü: ‡§Ø‡§π ‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ ‡§ï‡§π‡§æ‡§Å ‡§π‡•ã ‡§∞‡§π‡•Ä ‡§π‡•à?
‡§â‡§™‡§Ø‡•ã‡§ó‡§ï‡§∞‡•ç‡§§‡§æ: ‡§ó‡§≤‡•Ä ‡§®‡§Ç‡§¨‡§∞ 15, ‡§∏‡•á‡§ï‡•ç‡§ü‡§∞ 21, ‡§®‡•ã‡§è‡§°‡§æ ‡§Æ‡•á‡§Ç
‡§¨‡•â‡§ü: ‡§Ü‡§™‡§ï‡§æ ‡§®‡§æ‡§Æ ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à?
‡§â‡§™‡§Ø‡•ã‡§ó‡§ï‡§∞‡•ç‡§§‡§æ: ‡§∞‡§æ‡§ú ‡§ï‡•Å‡§Æ‡§æ‡§∞, ‡§Æ‡•ã‡§¨‡§æ‡§á‡§≤ ‡§®‡§Ç‡§¨‡§∞ 9876543210
"""
    
    print("\nüß™ Testing conversation analysis...")
    result = analyze_complaint_with_gemini(sample_conversation)
    print(f"Analysis Result: {result}")